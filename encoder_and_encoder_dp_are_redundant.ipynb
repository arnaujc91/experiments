{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder_and_encoder_dp_are_redundant.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGTmFSLsRTjqqu06w9Q7pn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/encoder_and_encoder_dp_are_redundant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pci8SNYn6fEF"
      },
      "source": [
        "!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install fastai==2.1.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odjKcES06hJD"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Y1WtWH7kAm"
      },
      "source": [
        "Load the weights of the pretrained model used for AWD_LSTM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97KYUJGr6hwQ",
        "outputId": "f7d9c581-38e8-48e4-f031-15e580036a07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = URLs.WT103_FWD\n",
        "model_path = untar_data(url , c_key='model')\n",
        "fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
        "wgts = torch.load(fnames[0], map_location = lambda storage,loc: storage)\n",
        "list(wgts.keys())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.encoder.weight',\n",
              " '0.encoder_dp.emb.weight',\n",
              " '0.rnns.0.weight_hh_l0_raw',\n",
              " '0.rnns.0.module.weight_ih_l0',\n",
              " '0.rnns.0.module.weight_hh_l0',\n",
              " '0.rnns.0.module.bias_ih_l0',\n",
              " '0.rnns.0.module.bias_hh_l0',\n",
              " '0.rnns.1.weight_hh_l0_raw',\n",
              " '0.rnns.1.module.weight_ih_l0',\n",
              " '0.rnns.1.module.weight_hh_l0',\n",
              " '0.rnns.1.module.bias_ih_l0',\n",
              " '0.rnns.1.module.bias_hh_l0',\n",
              " '0.rnns.2.weight_hh_l0_raw',\n",
              " '0.rnns.2.module.weight_ih_l0',\n",
              " '0.rnns.2.module.weight_hh_l0',\n",
              " '0.rnns.2.module.bias_ih_l0',\n",
              " '0.rnns.2.module.bias_hh_l0',\n",
              " '1.decoder.weight',\n",
              " '1.decoder.bias']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdWMy52y7Pe3"
      },
      "source": [
        "Both layers `0.encoder.weight` and `0.encoder_dp.emb.weight` have the **same** weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNeNIjrU7Lq2",
        "outputId": "6d49c219-b185-493d-d586-5d9294791f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.all(wgts['0.encoder.weight'] == wgts['0.encoder_dp.emb.weight'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwaF9lL28EkH"
      },
      "source": [
        "These weights come already from the pretrained model, so in the pretrained model there is already this redundancy. This issue is what I was trying to explain when I was talking about duplication of the layers, i.e. both embedding and embedding dropout should be in the same class and one of the two layers should be removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8nTLy-U8rBI"
      },
      "source": [
        "One of these two layers can be removed without problem and the AWS_LSTM model will keep still working fine. Then the splitting functions have to be modified accordinlgy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he0Kqu4183OT"
      },
      "source": [
        "def awd_lstm_lm_split(model):\n",
        "    \"Split a RNN `model` in groups for differential learning rates.\"\n",
        "    groups = [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n",
        "    # RIGHT NOW:\n",
        "    groups = L(groups + [nn.Sequential(model[0].encoder, model[0].encoder_dp, model[1])])\n",
        "    # WHAT WOULD BE WITH THE LAYER REMOVED:\n",
        "    groups = L(groups + [nn.Sequential(model[0].encoder, model[1])])\n",
        "    return groups.map(params)\n",
        "\n",
        "# Cell\n",
        "def awd_lstm_clas_split(model):\n",
        "    \"Split a RNN `model` in groups for differential learning rates.\"\n",
        "    # RIGHT NOW:\n",
        "    groups = [nn.Sequential(model[0].module.encoder, model[0].module.encoder_dp)]\n",
        "    # WHAT WOULD BE WITH THE LAYER REMOVED:\n",
        "    groups = [nn.Sequential(model[0].module.encoder)]\n",
        "    groups += [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].module.rnns, model[0].module.hidden_dps)]\n",
        "    groups = L(groups + [model[1]])\n",
        "    return groups.map(params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0NlPlGI8YxB"
      },
      "source": [
        "### Why I kept both layers in the code then?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWbSBpVS-EKu"
      },
      "source": [
        "This is because the method [language_model_learner](https://docs.fast.ai/text.learner#language_model_learner) automatically loads the pretrained weights from some server and this weights definitely need the model to have **both** layers if you want to load them. If you could modify this file (I guess I do not have access to it) then I can fully clean the code removing this redundant layer. What I need is just a new set of weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSYWnbg-_Bcj"
      },
      "source": [
        "del wgts['0.encoder_dp.emb.weight']\n",
        "new_weights = wgts"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBqC2kYL_OgU",
        "outputId": "b413cde2-3143-4d94-8d24-dc8713339b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(new_weights.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.encoder.weight',\n",
              " '0.rnns.0.weight_hh_l0_raw',\n",
              " '0.rnns.0.module.weight_ih_l0',\n",
              " '0.rnns.0.module.weight_hh_l0',\n",
              " '0.rnns.0.module.bias_ih_l0',\n",
              " '0.rnns.0.module.bias_hh_l0',\n",
              " '0.rnns.1.weight_hh_l0_raw',\n",
              " '0.rnns.1.module.weight_ih_l0',\n",
              " '0.rnns.1.module.weight_hh_l0',\n",
              " '0.rnns.1.module.bias_ih_l0',\n",
              " '0.rnns.1.module.bias_hh_l0',\n",
              " '0.rnns.2.weight_hh_l0_raw',\n",
              " '0.rnns.2.module.weight_ih_l0',\n",
              " '0.rnns.2.module.weight_hh_l0',\n",
              " '0.rnns.2.module.bias_ih_l0',\n",
              " '0.rnns.2.module.bias_hh_l0',\n",
              " '1.decoder.weight',\n",
              " '1.decoder.bias']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1VIeFJ_k3V"
      },
      "source": [
        "Otherwise the AWD_LSTM class will keep having these two lines of code:\n",
        "\n",
        "````\n",
        "self.encoder = EmbeddingDropout(vocab_sz, emb_sz, embed_p=embed_p, padding_idx=pad_token)\n",
        "self.encoder_dp = self.encoder\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8ri9AxWA0ob"
      },
      "source": [
        "As you can see now I solve the issue with declaring `self.encoder_dp` to be equal to `self.encoder` but I am never using it in the code. So it is a layer that it is there but is never used whatsoever. So first of all it does nothing and second it can confuse people who are looking at the code if they try to figure out what this layer is doing there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv0rGLDgBLiX"
      },
      "source": [
        "Therefore I would suggest that if you can update the weights of the pretrained model I can clean further the code to make full sense of this PR.\n",
        "\n",
        "Thanks a lot for all this great work Jeremy! :)"
      ]
    }
  ]
}