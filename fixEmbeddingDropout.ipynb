{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fixEmbeddingDropout.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWcStRsto9LgIM+IWDiABv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/fixEmbeddingDropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_ZLxzQc0tN"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4WvQ1uvdGRy"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGicrbydHCz"
      },
      "source": [
        "awd_lstm_original =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UCMbxrdpJD",
        "outputId": "ff2bb01b-371e-4679-893e-f4cff89aa8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_original); modules"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrLqDNKlhjk"
      },
      "source": [
        "### Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQlEn1DBd2Ul"
      },
      "source": [
        "1. `flatten_model` contains duplicated layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEOHf4qed6Qe",
        "outputId": "96b32b03-3abc-4727-fd02-56a89af8d5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  True\n",
            "Are the layers unique?  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJMp4JoYsIMz"
      },
      "source": [
        "This is because the function `flatten_model` takes the children of all present layers. In particular `EmbeddingDropout` has as children `nn.Embedding`, so instead of showing `EmbeddingDropout` as the second element of the list `modules` it shows *again* `Embedding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA5WA-Jsn-x",
        "outputId": "37539f57-8d15-4cb0-df04-91b433e3422e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm_original.encoder_dp.children()) == awd_lstm_original.encoder"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VKhwPNeA3S"
      },
      "source": [
        "2. The hooks are not fired for the *Embedding* layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLBvxRCad9uY",
        "outputId": "a17bf92f-5204-41e4-96a6-2e01d44112f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_original):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_original(torch.randint(3, (1,4)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0585,  0.0986, -0.0598,  0.0704,  0.0693],\n",
              "         [ 0.0783,  0.1497, -0.0994,  0.1160,  0.1130],\n",
              "         [ 0.0853,  0.1728, -0.1267,  0.1431,  0.1397],\n",
              "         [ 0.0879,  0.1821, -0.1455,  0.1586,  0.1554]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dys6SM1GgmDX"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOJA_sNMjeVb"
      },
      "source": [
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        # NEW CODE: \n",
        "        self.encoder = EmbeddingDropout(vocab_sz, emb_sz, embed_p=embed_p, padding_idx=pad_token)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        # PREVIOUS CODE:\n",
        "        # self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        # self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        # self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "        '''\n",
        "        IMPORTANT: As you can see previously the layer self.encoder was just used to create the layer self.encoder_dp.\n",
        "                   Now instead EmbeddingDropout directly inherits from nn.Embedding in order to avoid using self.encoder.\n",
        "                   Therefore now the code is more compact with the same functionality.\n",
        "        '''\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder(inp))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-R5hwzFjmGz"
      },
      "source": [
        "awd_lstm_modified=  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_-Ug_Mjpqx",
        "outputId": "7f16d3d8-9871-4b39-814f-2a9dfe01e5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_modified); modules"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[EmbeddingDropout(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dso0Eb_lX6P"
      },
      "source": [
        "Dupilcation of layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3735jCj7gl",
        "outputId": "967cc945-1cc6-4b5f-b5f5-b370a6f4a31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  False\n",
            "Are the layers unique?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hoM_WjClaIv"
      },
      "source": [
        "Hooks fired for the Embedding layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWvQ5WT1hvk_",
        "outputId": "577973e3-a720-4b60-ab81-832db4d8922d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_modified):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_modified(torch.randint(3, (1,4)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0048,  0.0082, -0.0706,  0.0583, -0.0391],\n",
              "         [-0.0160,  0.0125, -0.0939,  0.0878, -0.0568],\n",
              "         [-0.0276,  0.0097, -0.1013,  0.1029, -0.0654],\n",
              "         [-0.0388,  0.0047, -0.1059,  0.1119, -0.0689]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}