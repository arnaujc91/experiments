{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fixEmbeddingDropout.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMvppwyy5wrIRRP2mYspWPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/fixEmbeddingDropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_ZLxzQc0tN"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4WvQ1uvdGRy"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGicrbydHCz"
      },
      "source": [
        "awd_lstm_original =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UCMbxrdpJD",
        "outputId": "84ce48c1-9610-48c7-fcad-f10b904c3737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_original); modules"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrLqDNKlhjk"
      },
      "source": [
        "### Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQlEn1DBd2Ul"
      },
      "source": [
        "1. `flatten_model` contains duplicated layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEOHf4qed6Qe",
        "outputId": "cdcb63b4-a616-49aa-ff93-6bd1a7536607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  True\n",
            "Are the layers unique?  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VKhwPNeA3S"
      },
      "source": [
        "2. The hooks are not fired for the *Embedding* layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLBvxRCad9uY",
        "outputId": "6a489127-04eb-4bbb-9b93-03144e3ee881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_original):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_original(torch.randint(3, (1,4)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0858,  0.0119,  0.0125,  0.0354, -0.1397],\n",
              "         [-0.1311,  0.0134,  0.0043,  0.0531, -0.2374],\n",
              "         [-0.1546,  0.0124, -0.0076,  0.0595, -0.3074],\n",
              "         [-0.1673,  0.0103, -0.0164,  0.0572, -0.3556]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dys6SM1GgmDX"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOJA_sNMjeVb"
      },
      "source": [
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        # NEW CODE: \n",
        "        self.encoder_dp = EmbeddingDropout(vocab_sz, emb_sz, embed_p=embed_p, padding_idx=pad_token)\n",
        "        self.encoder_dp.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        # PREVIOUS CODE:\n",
        "        # self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        # self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        # self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "        '''\n",
        "        IMPORTANT: As you can see previously the layer self.encoder was just used to create the layer self.encoder_dp.\n",
        "                   Now instead EmbeddingDropout directly inherits from nn.Embedding in order to avoid using self.encoder.\n",
        "                   Therefore now the code is more compact with the same functionality.\n",
        "        '''\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder_dp(inp))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-R5hwzFjmGz"
      },
      "source": [
        "awd_lstm_modified=  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_-Ug_Mjpqx",
        "outputId": "f985e8ae-2754-405d-8a63-8de3f987c0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_modified); modules"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[EmbeddingDropout(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dso0Eb_lX6P"
      },
      "source": [
        "Dupilcation of layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3735jCj7gl",
        "outputId": "a27a960d-d6e3-4f34-8dcc-660225771b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  False\n",
            "Are the layers unique?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hoM_WjClaIv"
      },
      "source": [
        "Hooks fired for the Embedding layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWvQ5WT1hvk_",
        "outputId": "ff54fefd-d705-4888-a2c8-d39bca0e3604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_modified):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_modified(torch.randint(3, (1,4)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0545, -0.0419,  0.0573,  0.0768, -0.1884],\n",
              "         [ 0.0791, -0.0711,  0.0986,  0.1556, -0.2874],\n",
              "         [ 0.0873, -0.0869,  0.1286,  0.2193, -0.3454],\n",
              "         [ 0.0909, -0.1039,  0.1463,  0.2660, -0.3752]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}