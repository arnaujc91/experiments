{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddingDropout_function.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQY89k0omd233LPvb5cM76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/EmbeddingDropout_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_ZLxzQc0tN"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4WvQ1uvdGRy"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGicrbydHCz"
      },
      "source": [
        "awd_lstm_original =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UCMbxrdpJD",
        "outputId": "ff2bb01b-371e-4679-893e-f4cff89aa8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_original); modules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrLqDNKlhjk"
      },
      "source": [
        "### Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQlEn1DBd2Ul"
      },
      "source": [
        "1. `flatten_model` contains duplicated layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEOHf4qed6Qe",
        "outputId": "96b32b03-3abc-4727-fd02-56a89af8d5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  True\n",
            "Are the layers unique?  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJMp4JoYsIMz"
      },
      "source": [
        "This is because the function `flatten_model` takes the children of all present layers. In particular `EmbeddingDropout` has as children `nn.Embedding`, so instead of showing `EmbeddingDropout` as the second element of the list `modules` it shows *again* `Embedding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA5WA-Jsn-x",
        "outputId": "37539f57-8d15-4cb0-df04-91b433e3422e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm_original.encoder_dp.children()) == awd_lstm_original.encoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VKhwPNeA3S"
      },
      "source": [
        "2. The hooks are not fired for the *Embedding* layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLBvxRCad9uY",
        "outputId": "a17bf92f-5204-41e4-96a6-2e01d44112f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_original):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_original(torch.randint(3, (1,4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0585,  0.0986, -0.0598,  0.0704,  0.0693],\n",
              "         [ 0.0783,  0.1497, -0.0994,  0.1160,  0.1130],\n",
              "         [ 0.0853,  0.1728, -0.1267,  0.1431,  0.1397],\n",
              "         [ 0.0879,  0.1821, -0.1455,  0.1586,  0.1554]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dys6SM1GgmDX"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOJA_sNMjeVb"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def EmbeddingDropout(embed_p, x, training, scale=None):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    if training and embed_p != 0:\n",
        "        size = (x.size(0),1)\n",
        "        mask = dropout_mask(x.data, size, embed_p)\n",
        "        masked_embed = x * mask\n",
        "    else: masked_embed = x\n",
        "    if scale: masked_embed.mul_(scale)\n",
        "    return masked_embed\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        self.encoder_dp = partial(EmbeddingDropout, 0.5)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder_dp(self.encoder(inp), self.training))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-R5hwzFjmGz"
      },
      "source": [
        "awd_lstm_modified=  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_-Ug_Mjpqx",
        "outputId": "a2f6c949-30dc-472f-9d0f-83b6464efe43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_modified); modules"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dso0Eb_lX6P"
      },
      "source": [
        "Dupilcation of layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3735jCj7gl",
        "outputId": "967cc945-1cc6-4b5f-b5f5-b370a6f4a31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Are the first two layers the same? ', modules[0] == modules[1]) \n",
        "print('Are the layers unique? ', len(set(modules)) == len(modules))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are the first two layers the same?  False\n",
            "Are the layers unique?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hoM_WjClaIv"
      },
      "source": [
        "Hooks fired for the Embedding layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWvQ5WT1hvk_",
        "outputId": "58396f2f-e046-4f6b-948f-b205df008c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")\n",
        "\n",
        "for m in flatten_model(awd_lstm_modified):\n",
        "    if has_params(m):\n",
        "        m.register_forward_hook(hook_fn)\n",
        "\n",
        "awd_lstm_modified(torch.randint(3, (1,4)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- Embedding --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1600, -0.1292,  0.0209, -0.0205, -0.1360],\n",
              "         [ 0.2235, -0.1618,  0.0290, -0.0376, -0.1969],\n",
              "         [ 0.2460, -0.1673,  0.0326, -0.0491, -0.2228],\n",
              "         [ 0.2526, -0.1676,  0.0341, -0.0556, -0.2343]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}