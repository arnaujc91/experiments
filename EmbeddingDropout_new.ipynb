{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddingDropout_new.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIoWlIaAWYWBE2amQfuMUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/EmbeddingDropout_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZtPlQ3bXw2"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFr8fELebaMS"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9BFtdIneJYH"
      },
      "source": [
        "As you can see the class `EmbeddingDropout` is using `emb` (`self.encoder` for the AWD_LSTM class) just to fetch its attributes: *weight, scale_grad_by_freq, norm_type*, etc. It is much easier to sublcass `nn.Embedding` instead, then the attributes we are looking for are *already* inside the class and we do not have to create an instance of `nn.Embedding` and pass it to the constructor of `EmbeddingDropout` as currently is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HktiClyCbb1x"
      },
      "source": [
        "# CURRENT CODE\n",
        "class EmbeddingDropout(Module):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "\n",
        "    def __init__(self, emb, embed_p):\n",
        "      # self.emb is going to be an instance of the class 'nn.Embedding' \n",
        "        self.emb,self.embed_p = emb,embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.emb.weight.size(0),1)\n",
        "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.emb.weight * mask\n",
        "        else: masked_embed = self.emb.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.emb.padding_idx, -1), self.emb.max_norm,\n",
        "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
        "        \n",
        "# MY PROPOSAL\n",
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "      # Instead of passing an instance, that has to be previously created, from 'nn.Embedding', \n",
        "      # we directly inherit from 'nn.Embedding' such that what previously was 'self.emb' now is simpliy 'self'.\n",
        "      # Therefore we avoid the redundancy of creating previously an instance of 'nn.Embedding' \n",
        "      # and passing it as an argument to the constructor \n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEXsgFQ9mie0"
      },
      "source": [
        "**IMPORTANT**: I guess you wrote it that way because you wanted that people could load their own pretrained encoders/embeddings. You can still load a pretrained encoder as long as this is of the class `EmbeddingDropout` that I just created. The problem is: if you want to import an encoder that is an instance of `nn.Embedding` it will not work as far as my understanding goes. This may break compatibility with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqzdZX06ZrD"
      },
      "source": [
        "## What is the problem with the current code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_COemECr4j"
      },
      "source": [
        "### 1. First issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZL_lqe6rBn"
      },
      "source": [
        "First of all the function `flatten_model`, which is used to create the Hooks for a given model is not going to work as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnd4he66dGV"
      },
      "source": [
        "awd_lstm =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-wi8J_7YBT",
        "outputId": "af36e266-1ece-4376-893f-6e73f2a25b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "awd_lstm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AWD_LSTM(\n",
              "  (encoder): Embedding(3, 5, padding_idx=1)\n",
              "  (encoder_dp): EmbeddingDropout(\n",
              "    (emb): Embedding(3, 5, padding_idx=1)\n",
              "  )\n",
              "  (rnns): ModuleList(\n",
              "    (0): WeightDropout(\n",
              "      (module): LSTM(5, 6, batch_first=True)\n",
              "    )\n",
              "    (1): WeightDropout(\n",
              "      (module): LSTM(6, 5, batch_first=True)\n",
              "    )\n",
              "  )\n",
              "  (input_dp): RNNDropout()\n",
              "  (hidden_dps): ModuleList(\n",
              "    (0): RNNDropout()\n",
              "    (1): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CO7CdWvBfXz"
      },
      "source": [
        "You can see in the following line how the layer `Embedding` is **duplicated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpnF4fr6nu3",
        "outputId": "43aa5a32-b280-4d04-f50f-78cf2487c468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm); modules"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Kg8t0wBnwn"
      },
      "source": [
        "This is because `flatten_model` goes through all the layers and checks if they have children. The first layer is `encoder` and it does not have children, but the second layer is `encoder_dp` which indeed has children and the cildren is precisely `encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mETQ0E84CNgd",
        "outputId": "3dbb814a-0a74-4296-8a42-143dce0c4b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('encoder has children: ', awd_lstm.encoder.has_children )\n",
        "print('encoder_dp has children: ' ,awd_lstm.encoder_dp.has_children )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder has children:  False\n",
            "encoder_dp has children:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akdh8thiCa58"
      },
      "source": [
        "And because the children of `encoder_dp` is `encoder` this layer appears twice when we use `flatten_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOK9N9Z06-kG",
        "outputId": "040c7419-73f0-4169-9487-d48e4c7df930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm.encoder_dp.children()) == awd_lstm.encoder"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkme8IO9Cvyf"
      },
      "source": [
        "### 2. Second issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7aekORuDRJS"
      },
      "source": [
        "`flatten_model` does not contain the layer `EmbeddingDropout` and this is going to be a problem because when we use the forward method of `AWD_LSTM` this forward method calls the forward method of `EmbeddingDropout`  and not the one from `nn.Embedding`. As a consequence the hooks are not fired!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZc9efOESTv"
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKi2EMs7FoE",
        "outputId": "b4a2da5d-8169-49c3-d68e-66bdaaf5097a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0798, -0.0669, -0.1328,  0.0140,  0.1443],\n",
              "         [-0.1274, -0.0933, -0.1815,  0.0524,  0.2131],\n",
              "         [-0.1583, -0.0988, -0.1978,  0.0854,  0.2379],\n",
              "         [-0.1765, -0.0984, -0.2085,  0.1047,  0.2440]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJTUR-pEdpX"
      },
      "source": [
        "Eventhough I explicitly hooked the layer `encoder` its hooks do not get fired because its forward method is not called in the forward method of `AWD_LSTM`.\n",
        "Instead `AWD_LSTM` calls the forward method for `encoder_dp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQxR8rt8D8vp",
        "outputId": "860fc031-d57d-4b3b-ebbd-1e5da3be84a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "awd_lstm.encoder_dp.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1601, -0.0885, -0.2655,  0.1008,  0.2464],\n",
              "         [-0.1584, -0.0881, -0.2992,  0.1046,  0.2531],\n",
              "         [-0.1581, -0.0875, -0.3206,  0.1097,  0.2569],\n",
              "         [-0.1569, -0.0905, -0.3401,  0.1117,  0.2608]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAUL3uB5lbfF"
      },
      "source": [
        "## Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73hrJVTlgGs"
      },
      "source": [
        "I just see two possible solutions:\n",
        "1. Modify `flatten_model`\n",
        "2. Modifty `EmbeddingDropout`\n",
        "\n",
        "The modification I suggested suffers from the following problem: \n",
        "- If someone wants to load a pretrained `encoder` layer this has to be of the class `EmbeddingDropout` I just created, therefore breaking compatibility with PyTorch.\n",
        "\n",
        "Instead I suggest the following solution:\n",
        "\n",
        "- Downgrade the class `EmbeddingDropout` to a function instead of a class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO2s-axFmkjj"
      },
      "source": [
        "In the next cell I show you how I would modify the code in order to get the hooks fired and keep the `encoder` being from class `nn.Embedding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFkfwQxzm0te"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def EmbeddingDropout(embed_p, x, training, scale=None):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    if training and embed_p != 0:\n",
        "        size = (x.size(0),1)\n",
        "        mask = dropout_mask(x.data, size, embed_p)\n",
        "        masked_embed = x * mask\n",
        "    else: masked_embed = x\n",
        "    if scale: masked_embed.mul_(scale)\n",
        "    return masked_embed\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        # BEFORE: self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        self.encoder_dp = partial(EmbeddingDropout, 0.5)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        # BEFORE: output = self.input_dp(inp if from_embeds else self.encoder_dp(inp))\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder_dp(self.encoder(inp), self.training))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0sQ0XdfnZSV"
      },
      "source": [
        "awd_lstm_new =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dOOjZVoDkQ"
      },
      "source": [
        "1. Now there is no duplication of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI4vVMmUnab9",
        "outputId": "c50a7eb3-6657-4fce-f94e-f0a9cc7d300d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_new); modules"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSiVMesUoF76"
      },
      "source": [
        "2. The hooks for the encoder layer gets fired"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMlsW2Ftnjeu",
        "outputId": "a8cb131b-188a-4610-d2e3-d1c5ef69ac87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "awd_lstm_new.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm_new(torch.randint(3, (1,4)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- Embedding --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0439,  0.0046, -0.0066, -0.0590, -0.0050],\n",
              "         [-0.0639, -0.0044, -0.0116, -0.0901, -0.0070],\n",
              "         [-0.0728, -0.0163, -0.0162, -0.1083, -0.0085],\n",
              "         [-0.0760, -0.0288, -0.0199, -0.1181, -0.0116]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLzFDLjroMOH"
      },
      "source": [
        "I think now the functionality is the same and is compatible with PyTorch. So this could finally solve the issue. If you like this solution I will create a new PR.\n",
        "\n",
        "**NOTE**: The tests still will not pass because `encoder_dp` is now a function and not a class."
      ]
    }
  ]
}