{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddingDropout_new.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEIWHG7wBY4hVus2GOUmx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/EmbeddingDropout_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZtPlQ3bXw2",
        "outputId": "233b73a7-9755-4609-824d-051c71de7c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai==2.0.16 in /usr/local/lib/python3.6/dist-packages (2.0.16)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (1.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (3.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (1.4.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (19.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (20.4)\n",
            "Requirement already satisfied: fastcore>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (1.2.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (1.6.0+cu101)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (3.13)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (1.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (7.0.0)\n",
            "Requirement already satisfied: torchvision>=0.7 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.16) (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==2.0.16) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==2.0.16) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==2.0.16) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.16) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.16) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.16) (1.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.16) (3.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.16) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.16) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.16) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.16) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai==2.0.16) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==2.0.16) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->fastai==2.0.16) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai==2.0.16) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai==2.0.16) (3.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFr8fELebaMS"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9BFtdIneJYH"
      },
      "source": [
        "As you can see the class `EmbeddingDropout` is using `emb` (`self.encoder` for the AWD_LSTM class) just to fetch its attributes: *weight, scale_grad_by_freq, norm_type*, etc. It is much easier to sublcass `nn.Embedding` instead, then the attributes we are looking for are *already* inside the class and we do not have to create an instance of `nn.Embedding` and pass it to the constructor of `EmbeddingDropout` as currently is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HktiClyCbb1x"
      },
      "source": [
        "# CURRENT CODE\n",
        "class EmbeddingDropout(Module):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "\n",
        "    def __init__(self, emb, embed_p):\n",
        "      # self.emb is going to be an instance of the class 'nn.Embedding' \n",
        "        self.emb,self.embed_p = emb,embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.emb.weight.size(0),1)\n",
        "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.emb.weight * mask\n",
        "        else: masked_embed = self.emb.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.emb.padding_idx, -1), self.emb.max_norm,\n",
        "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
        "        \n",
        "# MY PROPOSAL\n",
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "      # Instead of passing an instance, that has to be previously created, from 'nn.Embedding', \n",
        "      # we directly inherit from 'nn.Embedding' such that what previously was 'self.emb' now is simpliy 'self'.\n",
        "      # Therefore we avoid the redundancy of creating previously an instance of 'nn.Embedding' \n",
        "      # and passing it as an argument to the constructor \n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEXsgFQ9mie0"
      },
      "source": [
        "**IMPORTANT**: I guess you wrote it that way because you wanted that people could load their own pretrained encoders/embeddings. You can still load a pretrained encoder as long as this is of the class `EmbeddingDropout` that I just created. The problem is: if you want to import an encoder that is an instance of `nn.Embedding` it will not work as far as my understanding goes. This may break compatibility with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqzdZX06ZrD"
      },
      "source": [
        "## What is the problem with the current code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_COemECr4j"
      },
      "source": [
        "### 1. First issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZL_lqe6rBn"
      },
      "source": [
        "First of all the function `flatten_model`, which is used to create the Hooks for a given model is not going to work as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnd4he66dGV"
      },
      "source": [
        "awd_lstm =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-wi8J_7YBT",
        "outputId": "0dbb8809-308e-4645-97e6-71444e3ea703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "awd_lstm"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AWD_LSTM(\n",
              "  (encoder): Embedding(3, 5, padding_idx=1)\n",
              "  (encoder_dp): EmbeddingDropout(\n",
              "    (emb): Embedding(3, 5, padding_idx=1)\n",
              "  )\n",
              "  (rnns): ModuleList(\n",
              "    (0): WeightDropout(\n",
              "      (module): LSTM(5, 6, batch_first=True)\n",
              "    )\n",
              "    (1): WeightDropout(\n",
              "      (module): LSTM(6, 5, batch_first=True)\n",
              "    )\n",
              "  )\n",
              "  (input_dp): RNNDropout()\n",
              "  (hidden_dps): ModuleList(\n",
              "    (0): RNNDropout()\n",
              "    (1): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CO7CdWvBfXz"
      },
      "source": [
        "You can see in the following line how the layer `Embedding` is **duplicated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpnF4fr6nu3",
        "outputId": "1e1cf7d6-77a4-412b-df98-2e1a891bac37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm); modules"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Kg8t0wBnwn"
      },
      "source": [
        "This is because `flatten_model` goes through all the layers and checks if they have children. The first layer is `encoder` and it does not have children, but the second layer is `encoder_dp` which indeed has children and the cildren is precisely `encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mETQ0E84CNgd",
        "outputId": "9b2a35a3-579e-41a4-8c19-19860375bdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('encoder has children: ', awd_lstm.encoder.has_children )\n",
        "print('encoder_dp has children: ' ,awd_lstm.encoder_dp.has_children )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder has children:  False\n",
            "encoder_dp has children:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akdh8thiCa58"
      },
      "source": [
        "And because the children of `encoder_dp` is `encoder` this layer appears twice when we use `flatten_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOK9N9Z06-kG",
        "outputId": "9e6ea2b7-3af6-4b34-c47c-02f6844ffa45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm.encoder_dp.children()) == awd_lstm.encoder"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkme8IO9Cvyf"
      },
      "source": [
        "### 2. Second issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7aekORuDRJS"
      },
      "source": [
        "`flatten_model` does not contain the layer `EmbeddingDropout` and this is going to be a problem because when we use the forward method of `AWD_LSTM` this forward method calls the forward method of `EmbeddingDropout`  and not the one from `nn.Embedding`. As a consequence the hooks are not fired!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZc9efOESTv"
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKi2EMs7FoE",
        "outputId": "424a0b39-2fd3-46e3-dd11-a8caae7ef8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0971, -0.0289, -0.0379, -0.1269, -0.0364],\n",
              "         [ 0.1373, -0.0516, -0.0667, -0.1806, -0.0620],\n",
              "         [ 0.1582, -0.0622, -0.0866, -0.2091, -0.0775],\n",
              "         [ 0.1641, -0.0726, -0.1000, -0.2262, -0.0861]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJTUR-pEdpX"
      },
      "source": [
        "Eventhough I explicitly hooked the layer `encoder` its hooks do not get fired because its forward method is not called in the forward method of `AWD_LSTM`.\n",
        "Instead `AWD_LSTM` calls the forward method for `encoder_dp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQxR8rt8D8vp",
        "outputId": "3ff2da24-5f97-4ae9-ff56-83a20661bc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "awd_lstm.encoder_dp.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1545, -0.0628, -0.1471, -0.2431, -0.0566],\n",
              "         [ 0.1544, -0.0514, -0.1732, -0.2352, -0.0483],\n",
              "         [ 0.1525, -0.0427, -0.1869, -0.2257, -0.0441],\n",
              "         [ 0.1517, -0.0365, -0.1928, -0.2212, -0.0399]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAUL3uB5lbfF"
      },
      "source": [
        "## Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73hrJVTlgGs"
      },
      "source": [
        "I just see two possible solutions:\n",
        "1. Modify `flatten_model`\n",
        "2. Modifty `EmbeddingDropout`\n",
        "\n",
        "The modification I suggested suffers from the following problem: \n",
        "- If someone wants to load a pretrained `encoder` layer this has to be of the class `EmbeddingDropout` I just created, therefore breaking compatibility with PyTorch.\n",
        "\n",
        "Instead I suggest the following solution:\n",
        "\n",
        "- Downgrade the class `EmbeddingDropout` to a function instead of a class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO2s-axFmkjj"
      },
      "source": [
        "In the next cell I show you how I would modify the code in order to get the hooks fired and keep the `encoder` being from class `nn.Embedding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFkfwQxzm0te"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def EmbeddingDropout(emb, embed_p, words, training, scale=None):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    if training and embed_p != 0:\n",
        "        size = (emb.weight.size(0),1)\n",
        "        mask = dropout_mask(emb.weight.data, size, embed_p)\n",
        "        masked_embed = emb.weight * mask\n",
        "    else: masked_embed = emb.weight\n",
        "    if scale: masked_embed.mul_(scale)\n",
        "    return F.embedding(words, masked_embed, ifnone(emb.padding_idx, -1), emb.max_norm,\n",
        "                        emb.norm_type, emb.scale_grad_by_freq, emb.sparse)\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        # BEFORE: self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        self.encoder_dp = partial(EmbeddingDropout, self.encoder, embed_p)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        # BEFORE: output = self.input_dp(inp if from_embeds else self.encoder_dp(inp))\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder_dp(inp, self.training))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0sQ0XdfnZSV"
      },
      "source": [
        "awd_lstm_new =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dOOjZVoDkQ"
      },
      "source": [
        "1. Now there is no duplication of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI4vVMmUnab9",
        "outputId": "29f3a921-0d8b-4f7f-bc6e-2a619c505c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_new); modules"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSiVMesUoF76"
      },
      "source": [
        "2. The hooks for the encoder layer gets fired"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMlsW2Ftnjeu",
        "outputId": "8c6dafc2-58ea-4c40-9ed3-901f125ea488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm_new.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm_new(torch.randint(3, (1,4)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0492,  0.0065, -0.0295,  0.0123,  0.0999],\n",
              "         [ 0.0722,  0.0096, -0.0463,  0.0096,  0.1649],\n",
              "         [ 0.0861,  0.0100, -0.0553,  0.0052,  0.2008],\n",
              "         [ 0.0945,  0.0093, -0.0607,  0.0027,  0.2199]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLzFDLjroMOH"
      },
      "source": [
        "I think now the functionality is the same and is compatible with PyTorch. So this could finally solve the issue. If you like this solution I will create a new PR.\n",
        "\n",
        "**NOTE**: The tests still will not pass because `encoder_dp` is now a function and not a class."
      ]
    }
  ]
}