{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddingDropout_new.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4zzXPlzPIFUx/9amQOfzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/EmbeddingDropout_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZtPlQ3bXw2"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFr8fELebaMS"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9BFtdIneJYH"
      },
      "source": [
        "As you can see the class `EmbeddingDropout` is using `emb` (`self.encoder` for the AWD_LSTM class) just to fetch its attributes: *weight, scale_grad_by_freq, norm_type*, etc. It is much easier to sublcass `nn.Embedding` instead, then the attributes we are looking for are *already* inside the class and we do not have to create an instance of `nn.Embedding` and pass it to the constructor of `EmbeddingDropout` as currently is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HktiClyCbb1x"
      },
      "source": [
        "# CURRENT CODE\n",
        "class EmbeddingDropout(Module):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "\n",
        "    def __init__(self, emb, embed_p):\n",
        "      # self.emb is going to be an instance of the class 'nn.Embedding' \n",
        "        self.emb,self.embed_p = emb,embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.emb.weight.size(0),1)\n",
        "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.emb.weight * mask\n",
        "        else: masked_embed = self.emb.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.emb.padding_idx, -1), self.emb.max_norm,\n",
        "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
        "        \n",
        "# MY PROPOSAL\n",
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "      # Instead of passing an instance, that has to be previously created, from 'nn.Embedding', \n",
        "      # we directly inherit from 'nn.Embedding' such that what previously was 'self.emb' now is simpliy 'self'.\n",
        "      # Therefore we avoid the redundancy of creating previously an instance of 'nn.Embedding' \n",
        "      # and passing it as an argument to the constructor \n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEXsgFQ9mie0"
      },
      "source": [
        "**IMPORTANT**: I guess you wrote it that way because you wanted that people could load their own pretrained encoders/embeddings. You can still load a pretrained encoder as long as this is of the class `EmbeddingDropout` that I just created. The problem is: if you want to import an encoder that is an instance of `nn.Embedding` it will not work as far as my understanding goes. This may break compatibility with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqzdZX06ZrD"
      },
      "source": [
        "## What is the problem with the current code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_COemECr4j"
      },
      "source": [
        "### 1. First issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZL_lqe6rBn"
      },
      "source": [
        "First of all the function `flatten_model`, which is used to create the Hooks for a given model is not going to work as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnd4he66dGV"
      },
      "source": [
        "awd_lstm =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-wi8J_7YBT",
        "outputId": "1f31ea08-35d7-45fb-a6cd-8a7ffc642686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "awd_lstm"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AWD_LSTM(\n",
              "  (encoder): Embedding(3, 5, padding_idx=1)\n",
              "  (encoder_dp): EmbeddingDropout(\n",
              "    (emb): Embedding(3, 5, padding_idx=1)\n",
              "  )\n",
              "  (rnns): ModuleList(\n",
              "    (0): WeightDropout(\n",
              "      (module): LSTM(5, 6, batch_first=True)\n",
              "    )\n",
              "    (1): WeightDropout(\n",
              "      (module): LSTM(6, 5, batch_first=True)\n",
              "    )\n",
              "  )\n",
              "  (input_dp): RNNDropout()\n",
              "  (hidden_dps): ModuleList(\n",
              "    (0): RNNDropout()\n",
              "    (1): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CO7CdWvBfXz"
      },
      "source": [
        "You can see in the following line how the layer `Embedding` is **duplicated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpnF4fr6nu3",
        "outputId": "74de4fa0-de33-4c1f-e2c2-3202bee34ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm); modules"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Kg8t0wBnwn"
      },
      "source": [
        "This is because `flatten_model` goes through all the layers and checks if they have children. The first layer is `encoder` and it does not have children, but the second layer is `encoder_dp` which indeed has children and the cildren is precisely `encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mETQ0E84CNgd",
        "outputId": "8a1d797a-174c-42fe-e1e8-0b67dbc9ebe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('encoder has children: ', awd_lstm.encoder.has_children )\n",
        "print('encoder_dp has children: ' ,awd_lstm.encoder_dp.has_children )"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder has children:  False\n",
            "encoder_dp has children:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akdh8thiCa58"
      },
      "source": [
        "And because the children of `encoder_dp` is `encoder` this layer appears twice when we use `flatten_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOK9N9Z06-kG",
        "outputId": "60a621a6-f8ce-48af-827e-8b2462778724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm.encoder_dp.children()) == awd_lstm.encoder"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkme8IO9Cvyf"
      },
      "source": [
        "### 2. Second issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7aekORuDRJS"
      },
      "source": [
        "`flatten_model` does not contain the layer `EmbeddingDropout` and this is going to be a problem because when we use the forward method of `AWD_LSTM` this forward method calls the forward method of `EmbeddingDropout`  and not the one from `nn.Embedding`. As a consequence the hooks are not fired!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZc9efOESTv"
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKi2EMs7FoE",
        "outputId": "2114eb66-ad16-4e08-99b6-fc38c0777539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.6479e-02,  3.3278e-05, -3.1192e-02,  5.2587e-02, -5.8732e-02],\n",
              "         [-9.3273e-02, -3.7378e-03, -2.3787e-02,  5.5440e-02, -8.4186e-02],\n",
              "         [-1.3828e-01, -6.5186e-03, -6.1914e-03,  5.5414e-02, -9.4652e-02],\n",
              "         [-1.7925e-01, -7.7681e-03,  1.1929e-02,  5.5593e-02, -1.0033e-01]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJTUR-pEdpX"
      },
      "source": [
        "Eventhough I explicitly hooked the layer `encoder` its hooks do not get fired because its forward method is not called in the forward method of `AWD_LSTM`.\n",
        "Instead `AWD_LSTM` calls the forward method for `encoder_dp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQxR8rt8D8vp",
        "outputId": "6718aa02-2e75-4d58-9c9b-ea189c4ae7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "awd_lstm.encoder_dp.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1902, -0.0110,  0.0290,  0.0780, -0.1060],\n",
              "         [-0.2059, -0.0089,  0.0358,  0.0850, -0.1116],\n",
              "         [-0.2215, -0.0065,  0.0431,  0.0940, -0.1162],\n",
              "         [-0.2369, -0.0037,  0.0466,  0.0938, -0.1204]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAUL3uB5lbfF"
      },
      "source": [
        "## Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73hrJVTlgGs"
      },
      "source": [
        "I just see two possible solutions:\n",
        "1. Modify `flatten_model`\n",
        "2. Modifty `EmbeddingDropout`\n",
        "\n",
        "The modification I suggested suffers from the following problem: \n",
        "- If someone wants to load a pretrained `encoder` layer this has to be of the class `EmbeddingDropout` I just created, therefore breaking compatibility with PyTorch.\n",
        "\n",
        "Instead I suggest the following solution:\n",
        "\n",
        "- Downgrade the class `EmbeddingDropout` to a function instead of a class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO2s-axFmkjj"
      },
      "source": [
        "In the next cell I show you how I would modify the code in order to get the hooks fired and keep the `encoder` being from class `nn.Embedding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFkfwQxzm0te"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def EmbeddingDropout(emb, embed_p, words, training, scale=None):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer.\"\n",
        "    if training and embed_p != 0:\n",
        "        size = (emb.weight.size(0),1)\n",
        "        mask = dropout_mask(emb.weight.data, size, embed_p)\n",
        "        masked_embed = emb.weight * mask\n",
        "    else: masked_embed = emb.weight\n",
        "    if scale: masked_embed.mul_(scale)\n",
        "    return F.embedding(words, masked_embed, ifnone(emb.padding_idx, -1), emb.max_norm,\n",
        "                        emb.norm_type, emb.scale_grad_by_freq, emb.sparse)\n",
        "        \n",
        "class AWD_LSTM(Module):\n",
        "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
        "                 weight_p=0.5, bidir=False):\n",
        "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
        "        self.bs = 1\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        # BEFORE: self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        self.encoder_dp = partial(EmbeddingDropout, self.encoder, embed_p)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
        "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def forward(self, inp, from_embeds=False):\n",
        "        bs,sl = inp.shape[:2] if from_embeds else inp.shape\n",
        "        if bs!=self.bs: self._change_hidden(bs)\n",
        "\n",
        "        # BEFORE: output = self.input_dp(inp if from_embeds else self.encoder_dp(inp))\n",
        "        output = self.input_dp(inp if from_embeds else self.encoder_dp(inp, self.training))\n",
        "        new_hidden = []\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            output, new_h = rnn(output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            if l != self.n_layers - 1: output = hid_dp(output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
        "        return output\n",
        "\n",
        "    def _change_hidden(self, bs):\n",
        "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
        "        self.bs = bs\n",
        "\n",
        "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
        "        \"Return one of the inner rnn\"\n",
        "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir)\n",
        "        return WeightDropout(rnn, weight_p)\n",
        "\n",
        "    def _one_hidden(self, l):\n",
        "        \"Return one hidden state\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
        "\n",
        "    def _change_one_hidden(self, l, bs):\n",
        "        if self.bs < bs:\n",
        "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
        "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
        "        return self.hidden[l]\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuZxrKeZxKi1"
      },
      "source": [
        "**NOTE**: Eventhough now EmbeddingDropout is a function and not a class, it keeps track of all changes that can happen on the fly of the object `self.encoder` so basically it behaves as if `self.encoder` was an attribute of the class that gets updated everytime that it changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0sQ0XdfnZSV"
      },
      "source": [
        "awd_lstm_new =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUHMfDNjxl6S",
        "outputId": "8ab7387c-2eb1-4791-c6d1-7d11ee9a335c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "emb_dp = partial(EmbeddingDropout, awd_lstm_new.encoder, 0.1)\n",
        "inp = torch.tensor([[0,1,2,2]])\n",
        "\n",
        "print('before:', emb_dp(inp, False))\n",
        "\n",
        "# Modify the layer awd_lstm_new.encoder\n",
        "awd_lstm_new.encoder.weight.data.fill_(2)\n",
        "print('')\n",
        "print('after:', emb_dp(inp, False))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: tensor([[[ 0.0069,  0.0021, -0.0639, -0.0390,  0.0881],\n",
            "         [ 0.0538, -0.0170,  0.0822,  0.0628, -0.0876],\n",
            "         [-0.0293, -0.0179, -0.0776,  0.0798, -0.0075],\n",
            "         [-0.0293, -0.0179, -0.0776,  0.0798, -0.0075]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "\n",
            "after: tensor([[[2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mQAooHAxz47"
      },
      "source": [
        "awd_lstm_new =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dOOjZVoDkQ"
      },
      "source": [
        "1. Now there is no duplication of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI4vVMmUnab9",
        "outputId": "90d29d98-f9b5-4343-a9b2-8cab37b7da90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm_new); modules"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSiVMesUoF76"
      },
      "source": [
        "2. The hooks for the encoder layer gets fired"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMlsW2Ftnjeu",
        "outputId": "b530cdaa-575a-4483-d649-68fb120ed766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm_new.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm_new(torch.randint(3, (1,4)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1104,  0.1290,  0.0286, -0.2030,  0.1597],\n",
              "         [ 0.1487,  0.1604,  0.0593, -0.3053,  0.2754],\n",
              "         [ 0.1664,  0.1457,  0.0912, -0.3567,  0.3436],\n",
              "         [ 0.1757,  0.1257,  0.1182, -0.3848,  0.3791]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLzFDLjroMOH"
      },
      "source": [
        "I think now the functionality is the same and is compatible with PyTorch. So this could finally solve the issue. If you like this solution I will create a new PR.\n",
        "\n",
        "**NOTE**: The tests still will not pass because `encoder_dp` is now a function and not a class."
      ]
    }
  ]
}