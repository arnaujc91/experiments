{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddingDropout_new.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcUAc66S0MFTA3jTVQiaTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaujc91/experiments/blob/main/EmbeddingDropout_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZtPlQ3bXw2"
      },
      "source": [
        "!pip install fastai==2.0.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFr8fELebaMS"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9BFtdIneJYH"
      },
      "source": [
        "As you can see the class `EmbeddingDropout` is using `emb` (`self.encoder` for the AWD_LSTM class) just to fetch its attributes: *weight, scale_grad_by_freq, norm_type*, etc. It is much easier to sublcass `nn.Embedding` instead, then the attributes we are looking for are *already* inside the class and we do not have to create an instance of `nn.Embedding` and pass it to the constructor of `EmbeddingDropout` as currently is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HktiClyCbb1x"
      },
      "source": [
        "# CURRENT CODE\n",
        "class EmbeddingDropout(Module):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "\n",
        "    def __init__(self, emb, embed_p):\n",
        "      # self.emb is going to be an instance of the class 'nn.Embedding' \n",
        "        self.emb,self.embed_p = emb,embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.emb.weight.size(0),1)\n",
        "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.emb.weight * mask\n",
        "        else: masked_embed = self.emb.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.emb.padding_idx, -1), self.emb.max_norm,\n",
        "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
        "        \n",
        "# MY PROPOSAL\n",
        "class EmbeddingDropout(nn.Embedding):\n",
        "    \"Apply dropout with probability `embed_p` to an embedding layer `emb`.\"\n",
        "    def __init__(self, *args, embed_p, **kwargs):\n",
        "      # Instead of passing an instance, that has to be previously created, from 'nn.Embedding', \n",
        "      # we directly inherit from 'nn.Embedding' such that what previously was 'self.emb' now is simpliy 'self'.\n",
        "      # Therefore we avoid the redundancy of creating previously an instance of 'nn.Embedding' \n",
        "      # and passing it as an argument to the constructor \n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.embed_p = embed_p\n",
        "\n",
        "    def forward(self, words, scale=None):\n",
        "        if self.training and self.embed_p != 0:\n",
        "            size = (self.weight.size(0),1)\n",
        "            mask = dropout_mask(self.weight.data, size, self.embed_p)\n",
        "            masked_embed = self.weight * mask\n",
        "        else: masked_embed = self.weight\n",
        "        if scale: masked_embed.mul_(scale)\n",
        "        return F.embedding(words, masked_embed, ifnone(self.padding_idx, -1), self.max_norm,\n",
        "                       self.norm_type, self.scale_grad_by_freq, self.sparse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEXsgFQ9mie0"
      },
      "source": [
        "**IMPORTANT**: Now there is just one layer `self.encoder` and not twice as before `self.encoder` and `self.encoder_dp`. Therefore the tests fail because they always expect two layers. These changes will affect also other parts of the code that expect two layers instead of one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqzdZX06ZrD"
      },
      "source": [
        "## What is the problem with the current code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_COemECr4j"
      },
      "source": [
        "### 1. First issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZL_lqe6rBn"
      },
      "source": [
        "First of all the function `flatten_model`, which is used to create the Hooks for a given model is not going to work as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnd4he66dGV"
      },
      "source": [
        "awd_lstm =  AWD_LSTM(vocab_sz=3,\n",
        "                  emb_sz=5,\n",
        "                  n_hid=6,\n",
        "                  n_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-wi8J_7YBT",
        "outputId": "1f31ea08-35d7-45fb-a6cd-8a7ffc642686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "awd_lstm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AWD_LSTM(\n",
              "  (encoder): Embedding(3, 5, padding_idx=1)\n",
              "  (encoder_dp): EmbeddingDropout(\n",
              "    (emb): Embedding(3, 5, padding_idx=1)\n",
              "  )\n",
              "  (rnns): ModuleList(\n",
              "    (0): WeightDropout(\n",
              "      (module): LSTM(5, 6, batch_first=True)\n",
              "    )\n",
              "    (1): WeightDropout(\n",
              "      (module): LSTM(6, 5, batch_first=True)\n",
              "    )\n",
              "  )\n",
              "  (input_dp): RNNDropout()\n",
              "  (hidden_dps): ModuleList(\n",
              "    (0): RNNDropout()\n",
              "    (1): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CO7CdWvBfXz"
      },
      "source": [
        "You can see in the following line how the layer `Embedding` is **duplicated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpnF4fr6nu3",
        "outputId": "74de4fa0-de33-4c1f-e2c2-3202bee34ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "modules = flatten_model(awd_lstm); modules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Embedding(3, 5, padding_idx=1),\n",
              " Embedding(3, 5, padding_idx=1),\n",
              " LSTM(5, 6, batch_first=True),\n",
              " ParameterModule(),\n",
              " LSTM(6, 5, batch_first=True),\n",
              " ParameterModule(),\n",
              " RNNDropout(),\n",
              " RNNDropout(),\n",
              " RNNDropout()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Kg8t0wBnwn"
      },
      "source": [
        "This is because `flatten_model` goes through all the layers and checks if they have children. The first layer is `encoder` and it does not have children, but the second layer is `encoder_dp` which indeed has children and the cildren is precisely `encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mETQ0E84CNgd",
        "outputId": "8a1d797a-174c-42fe-e1e8-0b67dbc9ebe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('encoder has children: ', awd_lstm.encoder.has_children )\n",
        "print('encoder_dp has children: ' ,awd_lstm.encoder_dp.has_children )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder has children:  False\n",
            "encoder_dp has children:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akdh8thiCa58"
      },
      "source": [
        "And because the children of `encoder_dp` is `encoder` this layer appears twice when we use `flatten_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOK9N9Z06-kG",
        "outputId": "60a621a6-f8ce-48af-827e-8b2462778724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(awd_lstm.encoder_dp.children()) == awd_lstm.encoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkme8IO9Cvyf"
      },
      "source": [
        "### 2. Second issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7aekORuDRJS"
      },
      "source": [
        "`flatten_model` does not contain the layer `EmbeddingDropout` and this is going to be a problem because when we use the forward method of `AWD_LSTM` this forward method calls the forward method of `EmbeddingDropout`  and not the one from `nn.Embedding`. As a consequence the hooks are not fired!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZc9efOESTv"
      },
      "source": [
        "def hook_fn(m, i, o):\n",
        "  print(f\"Working for layer: -- {m._get_name()} --\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKi2EMs7FoE",
        "outputId": "2114eb66-ad16-4e08-99b6-fc38c0777539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "awd_lstm.encoder.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.6479e-02,  3.3278e-05, -3.1192e-02,  5.2587e-02, -5.8732e-02],\n",
              "         [-9.3273e-02, -3.7378e-03, -2.3787e-02,  5.5440e-02, -8.4186e-02],\n",
              "         [-1.3828e-01, -6.5186e-03, -6.1914e-03,  5.5414e-02, -9.4652e-02],\n",
              "         [-1.7925e-01, -7.7681e-03,  1.1929e-02,  5.5593e-02, -1.0033e-01]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJTUR-pEdpX"
      },
      "source": [
        "Eventhough I explicitly hooked the layer `encoder` its hooks do not get fired because its forward method is not called in the forward method of `AWD_LSTM`.\n",
        "Instead `AWD_LSTM` calls the forward method for `encoder_dp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQxR8rt8D8vp",
        "outputId": "6718aa02-2e75-4d58-9c9b-ea189c4ae7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "awd_lstm.encoder_dp.register_forward_hook(hook_fn)\n",
        "awd_lstm(torch.randint(3, (1,4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working for layer: -- EmbeddingDropout --\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1902, -0.0110,  0.0290,  0.0780, -0.1060],\n",
              "         [-0.2059, -0.0089,  0.0358,  0.0850, -0.1116],\n",
              "         [-0.2215, -0.0065,  0.0431,  0.0940, -0.1162],\n",
              "         [-0.2369, -0.0037,  0.0466,  0.0938, -0.1204]]],\n",
              "       grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAUL3uB5lbfF"
      },
      "source": [
        "## Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73hrJVTlgGs"
      },
      "source": [
        "I just see two possible solutions:\n",
        "1. Modify `flatten_model`\n",
        "2. Modifty `EmbeddingDropout`\n",
        "\n",
        "So far I decided to modify the first one because I also find the code more compact. On the other hand, the modifications I propose here will have consquences in other parts of the code that assume `encoder_dp` and `encoder`, both, to be there. "
      ]
    }
  ]
}